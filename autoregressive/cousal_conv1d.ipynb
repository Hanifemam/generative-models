{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196480f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b01c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 84.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "raster_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.view(x.shape[0], -1)),  # (C,H,W) -> (C, H*W)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=raster_transform\n",
    ")\n",
    "\n",
    "dataset_test = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=raster_transform\n",
    ")\n",
    "\n",
    "train_loaded = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loaded = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3b04fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1922, 0.2196, 0.3373,  ..., 0.3569, 0.3490, 0.3255],\n",
       "         [0.1882, 0.2078, 0.3216,  ..., 0.3176, 0.3098, 0.2941],\n",
       "         [0.1922, 0.2118, 0.3294,  ..., 0.2824, 0.2784, 0.2667]]),\n",
       " 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, len(dataset_train) - 1)\n",
    "img, label = dataset_train[idx]\n",
    "img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84090de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7762d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal 1D convolution.\n",
    "    - Type B (A=False): y[t] depends on x[t] and past.\n",
    "    - Type A (A=True):  y[t] depends ONLY on x[t-1] and past (excludes current x[t]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, dilation=1, A=False, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.A = A\n",
    "\n",
    "        # Base causal left padding\n",
    "        base_pad = (kernel_size - 1) * dilation\n",
    "        # Type A needs one extra shift to exclude current input x[t]\n",
    "        self.left_pad = base_pad + (1 if A else 0)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,  # we do manual left padding only\n",
    "            dilation=dilation,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C_in, L)\n",
    "        returns:\n",
    "          - Type B: (B, C_out, L)\n",
    "          - Type A: (B, C_out, L) aligned so y[t] never uses x[t]\n",
    "        \"\"\"\n",
    "        # left pad only\n",
    "        x = F.pad(x, (self.left_pad, 0))\n",
    "        y = self.conv1d(x) \n",
    "\n",
    "        if self.A:\n",
    "            y = y[..., :-1]  # (B, C_out, L)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e2cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train_next_pixel_regression(model, train_loader, epochs=5, lr=1e-3, device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "        running_loss, total = 0.0, 0\n",
    "\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)  # (B, 3, 1024), values in [0,1] if ToTensor()\n",
    "\n",
    "            # teacher forcing: predict next step\n",
    "            x_in = x[:, :, :-1]  # (B, 3, 1023)\n",
    "            y_target = x[:, :, 1:]  # (B, 3, 1023)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(\n",
    "                x_in\n",
    "            )  # should be (B, 3, 1023) or (B, C_out, 1023) with a head to 3\n",
    "            loss = criterion(y_pred, y_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            total += x.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss={running_loss/total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc395f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4810cf663564fa591f6d507f453922a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.021092\n",
      "Epoch 2: train_loss=0.010093\n",
      "Epoch 3: train_loss=0.009042\n",
      "Epoch 4: train_loss=0.008753\n",
      "Epoch 5: train_loss=0.008621\n"
     ]
    }
   ],
   "source": [
    "model = CausalConv1d(3, 3, 5, 1)\n",
    "\n",
    "causal_conv1d = train_next_pixel_regression(model, train_loaded, epochs=5, lr=1e-3, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07516e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_image_raster(model, H=32, W=32, device=None, clamp=True, add_noise=0.0):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    L = H * W\n",
    "    x = torch.zeros(\n",
    "        1, 3, L, device=device\n",
    "    ) \n",
    "    x[:, :, 0] = torch.rand(1, 3, device=device)  \n",
    "\n",
    "    for t in range(0, L - 1):\n",
    "        x_in = x[:, :, : t + 1]  # (1, 3, t+1)\n",
    "        y_pred = model(x_in)  # (1, 3, t+1)\n",
    "        next_val = y_pred[:, :, -1]  # (1, 3)\n",
    "        if add_noise > 0:\n",
    "            next_val = next_val + add_noise * torch.randn_like(next_val)\n",
    "        if clamp:\n",
    "            next_val = next_val.clamp(0.0, 1.0)\n",
    "\n",
    "        x[:, :, t + 1] = next_val\n",
    "    img = x.view(1, 3, H, W).cpu()\n",
    "    plt.imshow(img.squeeze(0).permute(1, 2, 0))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2424ede9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1338, 0.2872, 0.2790,  ..., 0.4080, 0.4112, 0.4143],\n",
       "          [0.4172, 0.4200, 0.4226,  ..., 0.4606, 0.4612, 0.4618],\n",
       "          [0.4623, 0.4628, 0.4633,  ..., 0.4697, 0.4698, 0.4699],\n",
       "          ...,\n",
       "          [0.4713, 0.4713, 0.4713,  ..., 0.4713, 0.4713, 0.4713],\n",
       "          [0.4713, 0.4713, 0.4713,  ..., 0.4713, 0.4713, 0.4713],\n",
       "          [0.4713, 0.4713, 0.4713,  ..., 0.4713, 0.4713, 0.4713]],\n",
       "\n",
       "         [[0.8417, 0.8132, 0.6532,  ..., 0.4683, 0.4665, 0.4648],\n",
       "          [0.4633, 0.4620, 0.4607,  ..., 0.4518, 0.4518, 0.4519],\n",
       "          [0.4519, 0.4520, 0.4521,  ..., 0.4541, 0.4541, 0.4542],\n",
       "          ...,\n",
       "          [0.4561, 0.4561, 0.4561,  ..., 0.4561, 0.4561, 0.4561],\n",
       "          [0.4561, 0.4561, 0.4561,  ..., 0.4561, 0.4561, 0.4561],\n",
       "          [0.4561, 0.4561, 0.4561,  ..., 0.4561, 0.4561, 0.4561]],\n",
       "\n",
       "         [[0.1011, 0.3510, 0.2686,  ..., 0.2959, 0.2986, 0.3012],\n",
       "          [0.3038, 0.3064, 0.3090,  ..., 0.3594, 0.3606, 0.3618],\n",
       "          [0.3629, 0.3639, 0.3650,  ..., 0.3825, 0.3829, 0.3832],\n",
       "          ...,\n",
       "          [0.3921, 0.3921, 0.3921,  ..., 0.3921, 0.3921, 0.3921],\n",
       "          [0.3921, 0.3921, 0.3921,  ..., 0.3921, 0.3921, 0.3921],\n",
       "          [0.3921, 0.3921, 0.3921,  ..., 0.3921, 0.3921, 0.3921]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image_raster(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd1d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CausalConv1dV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal 1D convolution.\n",
    "    - Type B (A=False): y[t] depends on x[t] and past.\n",
    "    - Type A (A=True):  y[t] depends ONLY on x[t-1] and past (excludes current x[t]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, dilation=1, A=False, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.A = A\n",
    "\n",
    "        # Base causal left padding\n",
    "        base_pad = (kernel_size - 1) * dilation\n",
    "        # Type A needs one extra shift to exclude current input x[t]\n",
    "        self.left_pad = base_pad + (1 if A else 0)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,  # we do manual left padding only\n",
    "            dilation=dilation,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C_in, L)\n",
    "        returns:\n",
    "          - Type B: (B, C_out, L)\n",
    "          - Type A: (B, C_out, L) aligned so y[t] never uses x[t]\n",
    "        \"\"\"\n",
    "        # left pad only\n",
    "        x = F.pad(x, (self.left_pad, 0))\n",
    "        y = self.conv1d(x)\n",
    "\n",
    "        if self.A:\n",
    "            y = y[..., :-1]  # (B, C_out, L)\n",
    "\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
