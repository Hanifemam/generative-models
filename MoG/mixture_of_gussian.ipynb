{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812ee3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933f216b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd688982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "\n",
    "    def __init__(self, mode=\"train\", transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == \"train\":\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        elif mode == \"val\":\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21eba5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 350)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = Digits()\n",
    "test_data = Digits(mode=\"val\")\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879bee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dfad593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(7.5), np.float64(7.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "img = train_data[random.choice(range(len(train_data)))].reshape((8,8))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d26cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoG(nn.Module):\n",
    "    def __init__(self, D, K, uniform=False):\n",
    "        super(MoG, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.uniform = uniform\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "\n",
    "        # Parameters\n",
    "        self.mu = nn.Parameter(torch.randn(1, self.K, self.D) * 0.25 + 0.5)\n",
    "        self.log_var = nn.Parameter(-3.0 * torch.ones(1, self.K, self.D))\n",
    "\n",
    "        if self.uniform:\n",
    "            self.w = torch.zeros(1, self.K)\n",
    "            self.w.requires_grad = False\n",
    "        else:\n",
    "            self.w = nn.Parameter(torch.zeros(1, self.K))\n",
    "        self.PI = torch.from_numpy(np.asarray(np.pi))\n",
    "\n",
    "    def log_diag_normal(self, x, mu, log_var, reduction=\"sum\", dim=1):\n",
    "        log_p = (\n",
    "            -0.5 * torch.log(2.0 * self.PI)\n",
    "            - 0.5 * log_var\n",
    "            - 0.5 * torch.exp(-log_var) * (x.unsqueeze(1) - mu) ** 2.0\n",
    "        )\n",
    "        return log_p\n",
    "\n",
    "    def forward(self, x, reduction=\"mean\"):\n",
    "        log_pi = torch.log(\n",
    "            F.softmax(self.w, 1)\n",
    "        ) \n",
    "        log_N = torch.sum(self.log_diag_normal(x, self.mu, self.log_var), 2)\n",
    "        NLL_loss = -torch.logsumexp(log_pi + log_N, 1)  # B\n",
    "\n",
    "        if reduction == \"sum\":\n",
    "            return NLL_loss.sum()\n",
    "        elif reduction == \"mean\":\n",
    "            return NLL_loss.mean()\n",
    "        else:\n",
    "            raise ValueError(\"Either `sum` or `mean`.\")\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        x_sample = torch.empty(batch_size, self.D)\n",
    "\n",
    "        pi = F.softmax(\n",
    "            self.w, 1\n",
    "        )\n",
    "\n",
    "        indices = torch.multinomial(pi, batch_size, replacement=True).squeeze()\n",
    "\n",
    "        for n in range(batch_size):\n",
    "            indx = indices[n]\n",
    "            x_sample[n] = self.mu[0, indx] + torch.exp(\n",
    "                0.5 * self.log_var[0, indx]\n",
    "            ) * torch.randn(self.D)\n",
    "\n",
    "        return x_sample\n",
    "    \n",
    "    def log_prob(self, x, reduction='mean'):\n",
    "        with torch.no_grad():\n",
    "            log_pi = torch.log(F.softmax(self.w, 1)) \n",
    "            log_N = torch.sum(self.log_diag_normal(x, self.mu, self.log_var), 2)  \n",
    "        \n",
    "            log_prob = torch.logsumexp(log_pi + log_N,  1) \n",
    "            \n",
    "            if reduction == 'sum':\n",
    "                return log_prob.sum()\n",
    "            elif reduction == 'mean':\n",
    "                return log_prob.mean()\n",
    "            else:\n",
    "                raise ValueError('Either `sum` or `mean`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988e3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
